{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrtypes = [\"plain\", \"subredditname\", \"contextualized\"]\n",
    "subsets = [\"all\", \"askphysics\", \"explainlikeimfive\"]\n",
    "\n",
    "instrtype = instrtypes[1]\n",
    "subset = subsets[1]\n",
    "reward_model_name=f\"/projects/tir6/general/sachink/personalized-LM/2023/models/0923/reward_models/hf_model-7B_peft_reddit_{instrtype}_{subset}_2e-05_peft_last_checkpoint\"\n",
    "\n",
    "tokenizer_name = \"/projects/tir6/general/sachink/personalized-LM/2023/llama/hf_model-7B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, instrtype=None):\n",
    "    if instrtype is None:\n",
    "        new_examples = {\n",
    "            \"input_ids_j\": [],\n",
    "            \"attention_mask_j\": [],\n",
    "            \"input_ids_k\": [],\n",
    "            \"attention_mask_k\": [],\n",
    "        }\n",
    "        for question, response_j, response_k in zip(examples[\"question\"], examples[\"response_j\"], examples[\"response_k\"]):\n",
    "            tokenized_j = tokenizer(\"Question: \" + question + \"\\n\\nAnswer: \" + response_j, truncation=True)\n",
    "            tokenized_k = tokenizer(\"Question: \" + question + \"\\n\\nAnswer: \" + response_k, truncation=True)\n",
    "\n",
    "            new_examples[\"input_ids_j\"].append(tokenized_j[\"input_ids\"])\n",
    "            new_examples[\"attention_mask_j\"].append(tokenized_j[\"attention_mask\"])\n",
    "            new_examples[\"input_ids_k\"].append(tokenized_k[\"input_ids\"])\n",
    "            new_examples[\"attention_mask_k\"].append(tokenized_k[\"attention_mask\"])\n",
    "\n",
    "        return new_examples\n",
    "    else:\n",
    "        new_examples = {\n",
    "            \"input_ids_j\": [],\n",
    "            \"attention_mask_j\": [],\n",
    "            \"input_ids_k\": [],\n",
    "            \"attention_mask_k\": [],\n",
    "        }\n",
    "        for domain, question, response_j, response_k, label in zip(examples['domain'], examples[\"history\"], examples[\"human_ref_A\"], examples[\"human_ref_B\"], examples['labels']):\n",
    "            domain = domain.split(\"_\")[0]\n",
    "            if instrtype == \"subredditname\":\n",
    "                instruction = f\"Write a response to this reddit post in the following subreddit. SUBREDDIT: {domain}. \\n\\n POST: \"\n",
    "            elif instrtype == \"contextualized\":\n",
    "                instruction = f\"Write a response to this reddit post in the subreddit with the following description. SUBREDDIT: {SUBREDDIT2DESCRIPTION[domain]}. \\n\\n POST: \"\n",
    "            else:\n",
    "                instruction = f\"Write a response to this reddit post. \\n\\n POST: \"\n",
    "\n",
    "            if label == 0:\n",
    "                response_j, response_k = response_k, response_j\n",
    "            tokenized_j = tokenizer(instruction + question + \" \\n\\n COMMENT: \" + response_j, truncation=True)\n",
    "            tokenized_k = tokenizer(instruction + question + \" \\n\\n COMMENT: \" + response_k, truncation=True)\n",
    "\n",
    "            new_examples[\"input_ids_j\"].append(tokenized_j[\"input_ids\"])\n",
    "            new_examples[\"attention_mask_j\"].append(tokenized_j[\"attention_mask\"])\n",
    "            new_examples[\"input_ids_k\"].append(tokenized_k[\"input_ids\"])\n",
    "            new_examples[\"attention_mask_k\"].append(tokenized_k[\"attention_mask\"])\n",
    "\n",
    "        return new_examples\n",
    "\n",
    "preprocess_function_instr = partial(preprocess_function, instrtype=instrtype)\n",
    "# preprocess the dataset and filter out QAs that are longer than script_args.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007842779159545898,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b05b5fa7814eada8eee3d2ef5a08f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /projects/tir6/general/sachink/personalized-LM/2023/llama/hf_model-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=1, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=1, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model = AutoPeftModelForSequenceClassification.from_pretrained(reward_model_name, num_labels=1, load_in_8bit=True)#torch_dtype=torch.bfloat16)\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compute_loss(model, inputs, return_outputs=False):\n",
    "    rewards_j = model(input_ids=torch.LongTensor(inputs[\"input_ids_j\"]).to(\"cuda\"), attention_mask=torch.LongTensor(inputs[\"attention_mask_j\"]).to(\"cuda\"))[0]\n",
    "    rewards_k = model(input_ids=torch.LongTensor(inputs[\"input_ids_k\"]).to(\"cuda\"), attention_mask=torch.LongTensor(inputs[\"attention_mask_k\"]).to(\"cuda\"))[0]\n",
    "    # print(rewards_j,rewards_k)\n",
    "    loss = -torch.nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n",
    "    if return_outputs:\n",
    "        return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n",
    "    return loss\n",
    "\n",
    "eval_dataset = load_dataset(\"stanfordnlp/shp\", split=\"validation\", data_dir=subset)\n",
    "\n",
    "eval_original_columns = eval_dataset.column_names\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function_instr,\n",
    "    batched=True,\n",
    "    num_proc=24,\n",
    "    remove_columns=eval_original_columns,\n",
    ")\n",
    "eval_dataset = eval_dataset.filter(\n",
    "    lambda x: len(x[\"input_ids_j\"]) <= 512 and len(x[\"input_ids_k\"]) <= 512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.953125 8.0859375 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16796875 -1.7080078125 True\n",
      "0.0168304443359375 -1.7080078125 True\n",
      "0.2822265625 0.1298828125 True\n",
      "0.2822265625 -1.7080078125 True\n",
      "5.92578125 1.8330078125 True\n",
      "5.92578125 3.482421875 True\n",
      "8.1953125 2.91796875 True\n",
      "8.1953125 9.2578125 False\n",
      "8.1953125 5.09375 True\n",
      "8.1953125 3.29296875 True\n",
      "8.1953125 2.73046875 True\n",
      "8.1953125 1.90234375 True\n",
      "8.1953125 3.841796875 True\n",
      "8.1953125 1.3759765625 True\n",
      "2.91796875 9.2578125 False\n",
      "2.91796875 3.29296875 False\n",
      "9.2578125 3.29296875 True\n",
      "5.09375 3.29296875 True\n",
      "5.09375 2.73046875 True\n",
      "5.09375 3.841796875 True\n",
      "2.73046875 3.29296875 False\n",
      "2.73046875 3.841796875 False\n",
      "1.90234375 3.841796875 False\n",
      "7.70703125 6.56640625 True\n",
      "7.70703125 3.841796875 True\n",
      "7.70703125 1.3759765625 True\n",
      "6.56640625 1.3759765625 True\n",
      "5.10546875 1.3759765625 True\n",
      "3.466796875 2.650390625 True\n",
      "1.2373046875 6.81640625 False\n",
      "1.2373046875 2.650390625 False\n",
      "2.5078125 6.65625 False\n",
      "2.5078125 6.81640625 False\n",
      "2.5078125 2.650390625 False\n",
      "6.8125 6.65625 True\n",
      "6.8125 6.81640625 False\n",
      "6.8125 1.376953125 True\n",
      "6.8125 2.650390625 True\n",
      "5.26953125 6.65625 False\n",
      "5.26953125 6.81640625 False\n",
      "5.26953125 1.376953125 True\n",
      "5.26953125 2.650390625 True\n",
      "4.69921875 6.65625 False\n",
      "4.69921875 3.919921875 True\n",
      "4.69921875 0.354736328125 True\n",
      "4.69921875 6.81640625 False\n",
      "4.69921875 1.376953125 True\n",
      "4.69921875 0.08209228515625 True\n",
      "4.69921875 6.18359375 False\n",
      "4.69921875 2.650390625 True\n",
      "6.65625 6.81640625 False\n",
      "6.65625 2.650390625 True\n",
      "3.919921875 6.81640625 False\n",
      "3.919921875 1.376953125 True\n",
      "3.919921875 2.650390625 True\n",
      "0.354736328125 6.81640625 False\n",
      "0.354736328125 1.376953125 False\n",
      "0.354736328125 2.650390625 False\n",
      "4.97265625 6.81640625 False\n",
      "4.97265625 1.376953125 True\n",
      "4.97265625 0.08209228515625 True\n",
      "4.97265625 6.18359375 False\n",
      "4.97265625 3.634765625 True\n",
      "4.97265625 2.650390625 True\n",
      "4.97265625 1.2724609375 True\n",
      "4.97265625 1.443359375 True\n",
      "6.81640625 2.650390625 True\n",
      "1.376953125 2.650390625 False\n",
      "0.08209228515625 2.650390625 False\n",
      "6.18359375 2.650390625 True\n",
      "3.634765625 2.650390625 True\n",
      "3.634765625 1.2724609375 True\n",
      "2.546875 2.650390625 False\n",
      "2.546875 1.2724609375 True\n",
      "2.546875 1.443359375 True\n",
      "4.58203125 2.650390625 True\n",
      "4.58203125 1.2724609375 True\n",
      "4.58203125 1.443359375 True\n",
      "4.58203125 -0.0821533203125 True\n",
      "4.0703125 3.517578125 True\n",
      "4.2265625 1.1044921875 True\n",
      "3.53125 4.296875 False\n",
      "3.44921875 2.986328125 True\n",
      "6.515625 2.986328125 True\n",
      "1.2900390625 2.427734375 False\n",
      "6.1328125 5.5390625 True\n",
      "4.953125 -1.47265625 True\n",
      "2.720703125 -1.47265625 True\n",
      "2.720703125 0.468505859375 True\n",
      "1.12109375 -3.603515625 True\n",
      "1.12109375 0.346435546875 True\n",
      "2.94140625 4.1484375 False\n",
      "3.833984375 7.19921875 False\n",
      "3.779296875 7.19921875 False\n",
      "3.779296875 7.43359375 False\n",
      "3.642578125 7.43359375 False\n",
      "3.642578125 4.82421875 False\n",
      "3.642578125 2.041015625 True\n",
      "4.30078125 -4.6875 True\n",
      "4.30078125 -3.34765625 True\n",
      "4.30078125 -2.568359375 True\n",
      "-4.6875 -3.34765625 False\n",
      "-0.17822265625 0.029449462890625 False\n",
      "-0.89453125 0.85986328125 False\n",
      "-0.154296875 0.85986328125 False\n",
      "4.98046875 0.85986328125 True\n",
      "4.98046875 0.84765625 True\n",
      "6.546875 0.85986328125 True\n",
      "6.546875 0.84765625 True\n",
      "1.6279296875 0.85986328125 True\n",
      "1.6279296875 0.84765625 True\n",
      "7.01953125 -2.681640625 True\n",
      "7.01953125 -0.82470703125 True\n",
      "7.01953125 0.3388671875 True\n",
      "2.060546875 2.228515625 False\n",
      "-1.001953125 -3.41796875 True\n",
      "-1.001953125 2.69921875 False\n",
      "-1.3349609375 1.0205078125 False\n",
      "2.765625 1.0205078125 True\n",
      "-0.62548828125 4.39453125 False\n",
      "0.52734375 1.9521484375 False\n",
      "8.6796875 1.9521484375 True\n",
      "8.6796875 3.32421875 True\n",
      "8.6796875 0.1041259765625 True\n",
      "3.32421875 0.1041259765625 True\n",
      "5.078125 -2.02734375 True\n",
      "-0.99560546875 -2.02734375 True\n",
      "6.6484375 -2.02734375 True\n",
      "6.6484375 -0.9541015625 True\n",
      "1.2666015625 4.609375 False\n",
      "3.25 4.32421875 False\n",
      "3.25 -1.02734375 True\n",
      "3.25 2.587890625 True\n",
      "3.25 -0.204345703125 True\n",
      "3.25 4.6640625 False\n",
      "-2.650390625 4.32421875 False\n",
      "-2.650390625 -1.02734375 False\n",
      "-2.650390625 2.587890625 False\n",
      "-2.650390625 -0.204345703125 False\n",
      "-2.650390625 4.6640625 False\n",
      "-1.3134765625 2.587890625 False\n",
      "2.587890625 4.6640625 False\n",
      "-1.3134765625 -0.204345703125 False\n",
      "-1.3134765625 2.060546875 False\n",
      "-1.3134765625 4.6640625 False\n",
      "-1.3134765625 -0.007167816162109375 False\n",
      "-1.3134765625 3.87109375 False\n",
      "2.060546875 4.6640625 False\n",
      "1.0390625 -1.7841796875 True\n",
      "-1.9375 -1.7841796875 False\n",
      "-1.9375 -0.97607421875 False\n",
      "-1.9375 3.23828125 False\n",
      "2.171875 -0.97607421875 True\n",
      "2.171875 0.80810546875 True\n",
      "2.171875 -2.33984375 True\n",
      "2.171875 3.23828125 False\n",
      "0.80810546875 -2.33984375 True\n",
      "0.80810546875 3.23828125 False\n",
      "-1.953125 -2.33984375 True\n",
      "-1.953125 3.23828125 False\n",
      "-2.33984375 3.23828125 False\n",
      "1.98828125 3.23828125 False\n",
      "1.6865234375 1.09375 True\n",
      "1.6865234375 -2.685546875 True\n",
      "4.87109375 1.09375 True\n",
      "4.87109375 -2.685546875 True\n",
      "1.09375 -2.685546875 True\n",
      "5.3125 0.290283203125 True\n",
      "5.3125 -0.12445068359375 True\n",
      "-0.890625 0.385009765625 False\n",
      "-0.890625 4.32421875 False\n",
      "-0.890625 1.3701171875 False\n",
      "-0.890625 6.55078125 False\n",
      "-0.890625 1.8408203125 False\n",
      "0.385009765625 1.3701171875 False\n",
      "0.385009765625 6.55078125 False\n",
      "0.385009765625 1.8408203125 False\n",
      "4.32421875 1.3701171875 True\n",
      "4.32421875 6.55078125 False\n",
      "4.32421875 1.8408203125 True\n",
      "0.1947021484375 1.3701171875 False\n",
      "0.1947021484375 6.55078125 False\n",
      "0.1947021484375 1.8408203125 False\n",
      "0.1947021484375 7.34765625 False\n",
      "0.1947021484375 -0.90966796875 True\n",
      "0.1947021484375 4.3359375 False\n",
      "0.1947021484375 -0.7705078125 True\n",
      "1.373046875 0.87744140625 True\n",
      "1.373046875 -0.59619140625 True\n",
      "4.2890625 0.87744140625 True\n",
      "4.2890625 -0.59619140625 True\n",
      "0.87744140625 -0.59619140625 True\n",
      "0.53515625 -0.59619140625 True\n",
      "0.53515625 4.54296875 False\n",
      "0.53515625 2.259765625 False\n",
      "1.884765625 2.095703125 False\n",
      "6.890625 1.7158203125 True\n",
      "6.890625 2.095703125 True\n",
      "6.890625 1.494140625 True\n",
      "3.205078125 1.7158203125 True\n",
      "3.205078125 0.66650390625 True\n",
      "3.205078125 2.095703125 True\n",
      "3.205078125 5.63671875 False\n",
      "3.205078125 1.494140625 True\n",
      "1.7158203125 2.095703125 False\n",
      "0.66650390625 1.494140625 False\n",
      "5.09765625 5.63671875 False\n",
      "5.09765625 1.494140625 True\n",
      "5.09765625 6.77734375 False\n",
      "5.63671875 1.494140625 True\n",
      "3.171875 1.494140625 True\n",
      "3.171875 6.77734375 False\n",
      "6.59375 5.99609375 True\n",
      "4.1484375 5.99609375 False\n",
      "4.1484375 1.2294921875 True\n",
      "6.1015625 -1.255859375 True\n",
      "1.8544921875 -1.255859375 True\n",
      "1.1455078125 1.1767578125 False\n",
      "1.1455078125 -2.88671875 True\n",
      "3.484375 4.58203125 False\n",
      "4.58203125 -1.296875 True\n",
      "3.484375 -1.296875 True\n",
      "-0.9794921875 1.9833984375 False\n",
      "1.9833984375 3.154296875 False\n",
      "-0.9794921875 3.154296875 False\n",
      "2.29296875 1.509765625 True\n",
      "2.29296875 1.025390625 True\n",
      "2.3203125 1.509765625 True\n",
      "5.828125 1.94921875 True\n",
      "5.78125 6.71875 False\n",
      "5.78125 4.77734375 True\n",
      "5.78125 3.744140625 True\n",
      "0.5263671875 3.744140625 False\n",
      "4.53515625 3.744140625 True\n",
      "3.65625 7.0078125 False\n",
      "3.65625 5.8046875 False\n",
      "3.65625 5.9921875 False\n",
      "5.515625 -0.8125 True\n",
      "5.515625 -2.32421875 True\n",
      "-0.8125 -2.32421875 True\n",
      "5.703125 -2.32421875 True\n",
      "1.5458984375 -2.32421875 True\n",
      "-3.578125 -2.32421875 False\n",
      "-3.578125 2.087890625 False\n",
      "1.8896484375 8.46875 False\n",
      "1.8896484375 3.09765625 False\n",
      "4.0625 3.09765625 True\n",
      "2.55078125 4.5703125 False\n",
      "2.55078125 3.3125 False\n",
      "2.55078125 3.90625 False\n",
      "3.3125 4.5703125 False\n",
      "3.3125 3.90625 False\n",
      "-0.276611328125 -3.375 True\n",
      "-0.276611328125 -3.474609375 True\n",
      "9.1953125 4.08984375 True\n",
      "-0.495361328125 3.62890625 False\n",
      "1.2607421875 3.62890625 False\n",
      "3.208984375 3.62890625 False\n",
      "3.224609375 8.390625 False\n",
      "5.83984375 8.390625 False\n",
      "7.7265625 3.337890625 True\n",
      "7.62109375 3.337890625 True\n",
      "7.62109375 -0.619140625 True\n",
      "5.42578125 -2.951171875 True\n",
      "3.18359375 -1.044921875 True\n",
      "6.5234375 -3.109375 True\n",
      "6.5234375 -1.205078125 True\n",
      "6.5234375 1.3359375 True\n",
      "6.5234375 -3.390625 True\n",
      "-1.205078125 1.3359375 False\n",
      "4.96484375 -0.28662109375 True\n",
      "7.27734375 -0.869140625 True\n",
      "2.591796875 2.736328125 False\n",
      "2.591796875 3.015625 False\n",
      "2.591796875 -0.869140625 True\n",
      "2.736328125 -0.869140625 True\n",
      "1.904296875 5.40234375 False\n",
      "-2.12109375 -3.572265625 True\n",
      "-0.7177734375 -2.83984375 True\n",
      "-0.1160888671875 -2.83984375 True\n",
      "1.0009765625 -2.83984375 True\n",
      "3.77734375 1.4609375 True\n",
      "1.392578125 5.44140625 False\n",
      "1.392578125 7.0 False\n",
      "5.3671875 7.265625 False\n",
      "5.3671875 1.517578125 True\n",
      "5.3671875 7.87890625 False\n",
      "7.265625 1.517578125 True\n",
      "7.265625 7.87890625 False\n",
      "1.517578125 7.87890625 False\n",
      "3.490234375 7.87890625 False\n",
      "3.490234375 3.98828125 False\n",
      "3.490234375 5.06640625 False\n",
      "6.05859375 5.8828125 True\n",
      "6.05859375 -0.192138671875 True\n",
      "5.8828125 -0.192138671875 True\n",
      "6.0234375 2.435546875 True\n",
      "6.0234375 2.0625 True\n",
      "6.0234375 1.3681640625 True\n",
      "2.0625 1.3681640625 True\n",
      "6.8515625 0.62548828125 True\n",
      "6.8515625 2.41796875 True\n",
      "2.279296875 0.72119140625 True\n",
      "2.279296875 2.740234375 False\n",
      "6.2890625 3.26953125 True\n",
      "2.5859375 1.2685546875 True\n",
      "1.2685546875 7.421875 False\n",
      "1.2685546875 1.92578125 False\n",
      "2.5859375 7.421875 False\n",
      "2.5859375 1.92578125 True\n",
      "-1.451171875 0.6884765625 False\n",
      "7.20703125 5.61328125 True\n",
      "7.20703125 -2.197265625 True\n",
      "4.00390625 -0.175048828125 True\n",
      "4.00390625 -4.0546875 True\n",
      "7.44921875 4.2890625 True\n",
      "7.44921875 -0.48095703125 True\n",
      "-0.96826171875 2.13671875 False\n",
      "5.15625 2.052734375 True\n",
      "5.15625 3.201171875 True\n",
      "5.15625 -0.5009765625 True\n",
      "2.052734375 3.201171875 False\n",
      "2.052734375 -0.5009765625 True\n",
      "1.1162109375 -3.26171875 True\n",
      "-1.0693359375 -0.96484375 False\n",
      "2.06640625 -0.96484375 True\n",
      "0.7236328125 -0.28955078125 True\n",
      "0.7236328125 -1.298828125 True\n",
      "-0.361083984375 2.28515625 False\n",
      "4.7109375 -1.5703125 True\n",
      "4.7109375 -1.92578125 True\n",
      "4.7109375 -3.791015625 True\n",
      "-2.783203125 -1.5703125 False\n",
      "-2.783203125 -1.92578125 False\n",
      "-2.783203125 -3.791015625 True\n",
      "-1.92578125 -1.5703125 False\n",
      "-1.92578125 -3.791015625 True\n"
     ]
    }
   ],
   "source": [
    "reward_js = []\n",
    "reward_ks = []\n",
    "losses = []\n",
    "accurate = 0\n",
    "for i in range(len(eval_dataset)):\n",
    "    loss, rewards = compute_loss(reward_model, eval_dataset[i:i+1], return_outputs=True)\n",
    "    losses.append(loss)\n",
    "    rj = rewards['rewards_j'][0][0].item()\n",
    "    rk = rewards['rewards_k'][0][0].item()\n",
    "    reward_js.append(rj)\n",
    "    reward_ks.append(rk)\n",
    "    accurate += rj > rk\n",
    "    print(rj, rk, rj > rk)\n",
    "print(accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5798816568047337"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sum([rj > rk for rj, rk in zip(reward_js, reward_ks)])/ len(reward_js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
